{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59891f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP: DATASET & MODEL DEFINITION\n",
    "# (Must match Training script exactly)\n",
    "# ==========================================\n",
    "\n",
    "class StanfordCarsDataset(Dataset):\n",
    "    def __init__(self, root_dir, mat_file, image_folder, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.annotations = scipy.io.loadmat(mat_file)\n",
    "        raw_samples = self.annotations['annotations'][0]\n",
    "        \n",
    "        # Filtering logic (Must match training)\n",
    "        self.samples = []\n",
    "        for sample in raw_samples:\n",
    "            img_name = sample[-1][0]\n",
    "            img_path = os.path.join(self.root_dir, self.image_folder, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                self.samples.append(sample)\n",
    "                \n",
    "        self.classes = [str(i) for i in range(196)] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.samples[idx]\n",
    "        img_name = ann[-1][0] \n",
    "        label = ann[-2][0][0] - 1  \n",
    "        img_path = os.path.join(self.root_dir, self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "def get_resnet_model(num_classes=196):\n",
    "    # Load architecture without weights\n",
    "    model = models.resnet50(weights=None) \n",
    "    \n",
    "    # Re-create the head (MATCHING THE NEW TRAINING CONFIG)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 1024), # Updated to 1024\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),              # Updated to 0.5\n",
    "        nn.Linear(1024, num_classes)  # Updated to 1024\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_validation_loader(data_root, batch_size=32):\n",
    "    # Standard ImageNet normalization\n",
    "    stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    valid_tfms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*stats)\n",
    "    ])\n",
    "\n",
    "    # Path logic (Robust check)\n",
    "    possible_paths = [\n",
    "        os.path.join(data_root, 'car_devkit', 'devkit', 'cars_train_annos.mat'),\n",
    "        os.path.join(data_root, 'car_devkit', 'cars_train_annos.mat'),\n",
    "        os.path.join(data_root, 'cars_train_annos.mat')\n",
    "    ]\n",
    "    mat_path = next((p for p in possible_paths if os.path.exists(p)), None)\n",
    "    \n",
    "    if not mat_path:\n",
    "        raise FileNotFoundError(\"Could not find annotation .mat file.\")\n",
    "\n",
    "    full_dataset = StanfordCarsDataset(\n",
    "        root_dir=data_root, \n",
    "        mat_file=mat_path,\n",
    "        image_folder=r'cars_train/cars_train', # Ensure this matches your folder structure\n",
    "        transform=valid_tfms \n",
    "    )\n",
    "\n",
    "    # Re-create split with random_state=42 to get the SAME validation set as training\n",
    "    labels = [full_dataset.samples[i][-2][0][0] for i in range(len(full_dataset))]\n",
    "    _, val_indices = train_test_split(\n",
    "        range(len(full_dataset)), \n",
    "        test_size=0.2, \n",
    "        stratify=labels, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    val_set = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    return DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# ==========================================\n",
    "# 2. EVALUATION LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def evaluate_model(model_path, data_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"Loading Validation Data...\")\n",
    "    val_loader = get_validation_loader(data_path)\n",
    "    \n",
    "    # 2. Load Model\n",
    "    print(f\"Loading Model from {model_path}...\")\n",
    "    model = get_resnet_model(num_classes=196)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights: {e}\")\n",
    "        return\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    print(\"Running Inference...\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1) \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # 4. Metrics & Visualization\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"       TEST RESULTS       \")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    # --- Accuracy ---\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Overall Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    # --- Classification Report (Pandas Version) ---\n",
    "    print(\"\\nGenerating Detailed Report...\")\n",
    "    report_dict = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    \n",
    "    # Print Top 10 Best Classes (by F1-score)\n",
    "    print(\"\\n--- Top 5 Best Classified Cars ---\")\n",
    "    print(report_df.sort_values(by='f1-score', ascending=False).head(5))\n",
    "\n",
    "    # Print Top 10 Worst Classes\n",
    "    print(\"\\n--- Top 5 Worst Classified Cars ---\")\n",
    "    # Exclude the average rows\n",
    "    classes_only = report_df.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
    "    print(classes_only.sort_values(by='f1-score', ascending=True).head(5))\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    print(\"\\nGenerating Confusion Matrix (First 20 classes)...\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm[:20, :20], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix (First 20 Classes)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- ROC & AUC (One-vs-Rest) ---\n",
    "    print(\"\\nCalculating ROC & AUC...\")\n",
    "    y_test_bin = label_binarize(all_labels, classes=range(196))\n",
    "    n_classes = y_test_bin.shape[1]\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Calculate for all classes\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'Micro-average ROC (area = {roc_auc[\"micro\"]:0.2f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    # Plot 3 random classes\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(3), colors): \n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'Class {i} ROC (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 3. RUN EVALUATION\n",
    "# ==========================================\n",
    "\n",
    "# CONFIGURATION\n",
    "MODEL_PATH = \"resnet50_stanford_cars.pth\" \n",
    "DATA_PATH = r\"D:/deep_learning/stanford-cars-dataset\" \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists(MODEL_PATH) and os.path.exists(DATA_PATH):\n",
    "        evaluate_model(MODEL_PATH, DATA_PATH)\n",
    "    else:\n",
    "        print(\"Error: Could not find model or dataset path.\")\n",
    "        print(f\"Checking Model: {MODEL_PATH} -> {os.path.exists(MODEL_PATH)}\")\n",
    "        print(f\"Checking Data: {DATA_PATH} -> {os.path.exists(DATA_PATH)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
