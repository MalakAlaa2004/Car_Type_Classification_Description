{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06243ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Hugging Face Data Pipeline...\n",
      "Loading data from Hugging Face Hub...\n",
      " Loading 'tanganke/stanford_cars' from Hugging Face Hub...\n",
      "âœ… Data Split: 6515 Train | 1629 Val | 8041 Test\n",
      "Initializing Inception V1 (GoogLeNet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training on cuda...\n",
      "Epoch 1/30 | Train Loss: 8.5357 | Train Acc: 1.24% | Val Loss: 5.1678 | Val Acc: 1.35%\n",
      "Epoch 2/30 | Train Loss: 8.1163 | Train Acc: 3.94% | Val Loss: 4.5336 | Val Acc: 7.00%\n",
      "Epoch 3/30 | Train Loss: 7.4344 | Train Acc: 8.58% | Val Loss: 3.7890 | Val Acc: 15.35%\n",
      "Epoch 4/30 | Train Loss: 6.7890 | Train Acc: 14.89% | Val Loss: 3.1853 | Val Acc: 26.21%\n",
      "Epoch 5/30 | Train Loss: 6.2688 | Train Acc: 22.46% | Val Loss: 2.7322 | Val Acc: 34.25%\n",
      "Epoch 6/30 | Train Loss: 5.8048 | Train Acc: 30.44% | Val Loss: 2.3769 | Val Acc: 43.03%\n",
      "Epoch 7/30 | Train Loss: 5.4418 | Train Acc: 38.08% | Val Loss: 2.1030 | Val Acc: 47.02%\n",
      "Epoch 8/30 | Train Loss: 5.1428 | Train Acc: 44.51% | Val Loss: 1.8683 | Val Acc: 52.67%\n",
      "Epoch 9/30 | Train Loss: 4.8604 | Train Acc: 50.44% | Val Loss: 1.7215 | Val Acc: 55.13%\n",
      "Epoch 10/30 | Train Loss: 4.6291 | Train Acc: 55.66% | Val Loss: 1.5362 | Val Acc: 59.91%\n",
      "Epoch 11/30 | Train Loss: 4.4227 | Train Acc: 60.83% | Val Loss: 1.4383 | Val Acc: 61.39%\n",
      "Epoch 12/30 | Train Loss: 4.2521 | Train Acc: 65.17% | Val Loss: 1.3402 | Val Acc: 64.21%\n",
      "Epoch 13/30 | Train Loss: 4.0977 | Train Acc: 69.04% | Val Loss: 1.2769 | Val Acc: 65.25%\n",
      "Epoch 14/30 | Train Loss: 3.9694 | Train Acc: 71.86% | Val Loss: 1.2032 | Val Acc: 66.97%\n",
      "Epoch 15/30 | Train Loss: 3.8445 | Train Acc: 75.06% | Val Loss: 1.1467 | Val Acc: 67.40%\n",
      "Epoch 16/30 | Train Loss: 3.7473 | Train Acc: 77.04% | Val Loss: 1.0972 | Val Acc: 68.69%\n",
      "Epoch 17/30 | Train Loss: 3.6534 | Train Acc: 79.80% | Val Loss: 1.0493 | Val Acc: 69.43%\n",
      "Epoch 18/30 | Train Loss: 3.5753 | Train Acc: 81.24% | Val Loss: 1.0481 | Val Acc: 70.47%\n",
      "Epoch 19/30 | Train Loss: 3.5144 | Train Acc: 82.99% | Val Loss: 1.0587 | Val Acc: 69.67%\n",
      "Epoch 20/30 | Train Loss: 3.4383 | Train Acc: 84.70% | Val Loss: 1.0349 | Val Acc: 70.10%\n",
      "Epoch 21/30 | Train Loss: 3.3717 | Train Acc: 86.91% | Val Loss: 0.9784 | Val Acc: 70.90%\n",
      "Epoch 22/30 | Train Loss: 3.3294 | Train Acc: 87.28% | Val Loss: 0.9994 | Val Acc: 71.15%\n",
      "Epoch 23/30 | Train Loss: 3.2764 | Train Acc: 88.69% | Val Loss: 0.9902 | Val Acc: 72.13%\n",
      "Epoch 24/30 | Train Loss: 3.2289 | Train Acc: 90.25% | Val Loss: 0.9824 | Val Acc: 72.31%\n",
      "Epoch 25/30 | Train Loss: 3.1947 | Train Acc: 90.41% | Val Loss: 1.0130 | Val Acc: 71.76%\n",
      "Epoch 26/30 | Train Loss: 3.1277 | Train Acc: 91.82% | Val Loss: 0.9437 | Val Acc: 73.73%\n",
      "Epoch 27/30 | Train Loss: 3.1165 | Train Acc: 92.46% | Val Loss: 0.9300 | Val Acc: 73.91%\n",
      "Epoch 28/30 | Train Loss: 3.0894 | Train Acc: 93.55% | Val Loss: 0.9148 | Val Acc: 74.16%\n",
      "Epoch 29/30 | Train Loss: 3.0848 | Train Acc: 93.15% | Val Loss: 0.9016 | Val Acc: 74.22%\n",
      "Epoch 30/30 | Train Loss: 3.0778 | Train Acc: 93.88% | Val Loss: 0.8927 | Val Acc: 74.34%\n",
      "Training loop finished successfully!\n",
      "Saving model...\n",
      "Model saved as 'inception_v1_stanford_cars.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from data_preprocessing import get_dataloaders\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_SAVE_PATH = \"inception_v1_stanford_cars.pth\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 196\n",
    "NUM_EPOCHS = 30\n",
    "IMG_SIZE = 299  # Inception requires 299x299 input\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL ARCHITECTURE\n",
    "# ==========================================\n",
    "def get_inception_model(num_classes=196):\n",
    "    # Load Pre-trained Inception V1 (GoogLeNet) with auxiliary classifiers enabled\n",
    "    model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1, aux_logits=True)\n",
    "\n",
    "    # --- STRATEGY: FINE-TUNING ---\n",
    "    # 1. Freeze the early layers (generic features like lines/edges)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # 2. Unfreeze the last inception modules for better feature learning\n",
    "    # GoogLeNet has inception4 and inception5 modules\n",
    "    for param in model.inception4e.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.inception5a.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.inception5b.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # 3. Replace the Final Classifier Head\n",
    "    in_features = model.fc.in_features  # GoogLeNet has 1024 features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1024, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Replace auxiliary classifiers to match our number of classes\n",
    "    model.aux1.fc2 = nn.Linear(model.aux1.fc2.in_features, num_classes)\n",
    "    model.aux2.fc2 = nn.Linear(model.aux2.fc2.in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING SKELETON\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Initializing Hugging Face Data Pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load Data using HF pipeline (handles downloading, splitting, and transforms)\n",
    "        print(\"Loading data from Hugging Face Hub...\")\n",
    "        train_dl, val_dl, test_dl = get_dataloaders(\n",
    "            batch_size=BATCH_SIZE, \n",
    "            img_size=IMG_SIZE,  # Inception requires 299x299\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        print(\"Initializing Inception V1 (GoogLeNet)...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = get_inception_model(num_classes=NUM_CLASSES).to(device)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # Add weight_decay=1e-4\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "        \n",
    "        # Learning Rate Scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "        \n",
    "        print(f\"Starting Training on {device}...\")\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # ==========================================\n",
    "            # TRAINING PHASE\n",
    "            # ==========================================\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0 \n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_dl:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # GoogLeNet returns (outputs, aux2, aux1) during training when aux_logits=True\n",
    "                outputs = model(images)\n",
    "                \n",
    "                if isinstance(outputs, tuple):\n",
    "                    # Training mode: unpack main output and auxiliary outputs\n",
    "                    main_output, aux2_output, aux1_output = outputs\n",
    "                    loss1 = criterion(main_output, labels)\n",
    "                    loss2 = criterion(aux2_output, labels)\n",
    "                    loss3 = criterion(aux1_output, labels)\n",
    "                    # Combined loss: main output weighted more heavily\n",
    "                    loss = loss1 + 0.3 * loss2 + 0.3 * loss3\n",
    "                    outputs = main_output  # Use main output for accuracy calculation\n",
    "                else:\n",
    "                    # Inference mode: single output\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Track Training Accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_epoch_loss = train_loss / len(train_dl)\n",
    "            train_epoch_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            # ==========================================\n",
    "            # VALIDATION PHASE\n",
    "            # ==========================================\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_dl:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    # In eval mode, model returns single output\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # Track Validation Accuracy\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_epoch_loss = val_loss / len(val_dl)\n",
    "            val_epoch_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "                  f\"Train Loss: {train_epoch_loss:.4f} | Train Acc: {train_epoch_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_epoch_acc:.2f}%\")\n",
    "            \n",
    "            # Update LR based on validation loss\n",
    "            scheduler.step(val_epoch_loss)\n",
    "            \n",
    "        print(\"Training loop finished successfully!\")\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Model saved as '{MODEL_SAVE_PATH}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n An error occurred during execution:\")\n",
    "        print(e)\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ee18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
