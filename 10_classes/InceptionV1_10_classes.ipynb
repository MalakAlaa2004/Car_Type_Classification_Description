{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5040390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ Inception V1 (GoogLeNet) Training on 20 Random Classes\n",
      "============================================================\n",
      "\n",
      "üì¶ Loading data from Hugging Face Hub...\n",
      "üöÄ Loading 'tanganke/stanford_cars' from Hugging Face Hub...\n",
      "üìä Total classes in dataset: 196\n",
      "üéØ Selected 10 random classes: [6, 26, 28, 35, 57, 62, 70, 163, 188, 189]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8144/8144 [00:44<00:00, 183.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filtered dataset size: 410 samples\n",
      "üìä Total classes in dataset: 196\n",
      "üéØ Selected 10 random classes: [6, 26, 28, 35, 57, 62, 70, 163, 188, 189]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8041/8041 [00:35<00:00, 229.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filtered dataset size: 406 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8041/8041 [00:36<00:00, 220.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Split: 328 Train | 82 Val | 406 Test\n",
      "üìå Classes remapped to range: 0-9\n",
      "\n",
      "üéØ Selected Classes: [6, 26, 28, 35, 57, 62, 70, 163, 188, 189]\n",
      "üìä Number of Classes: 10\n",
      "\n",
      "ü§ñ Initializing Inception V1 (GoogLeNet)...\n",
      "üíª Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Starting Training for 30 epochs...\n",
      "============================================================\n",
      "Epoch [ 1/30] | Train Loss: 3.7806 | Train Acc: 11.89% | Val Loss: 2.2804 | Val Acc: 10.98%\n",
      "  ‚úÖ Best model saved! (Val Acc: 10.98%)\n",
      "Epoch [ 2/30] | Train Loss: 3.6778 | Train Acc: 22.87% | Val Loss: 2.2573 | Val Acc: 13.41%\n",
      "  ‚úÖ Best model saved! (Val Acc: 13.41%)\n",
      "Epoch [ 3/30] | Train Loss: 3.6267 | Train Acc: 27.44% | Val Loss: 2.1970 | Val Acc: 26.83%\n",
      "  ‚úÖ Best model saved! (Val Acc: 26.83%)\n",
      "Epoch [ 4/30] | Train Loss: 3.5339 | Train Acc: 38.41% | Val Loss: 2.1050 | Val Acc: 32.93%\n",
      "  ‚úÖ Best model saved! (Val Acc: 32.93%)\n",
      "Epoch [ 5/30] | Train Loss: 3.3926 | Train Acc: 49.39% | Val Loss: 1.9625 | Val Acc: 47.56%\n",
      "  ‚úÖ Best model saved! (Val Acc: 47.56%)\n",
      "Epoch [ 6/30] | Train Loss: 3.2837 | Train Acc: 59.15% | Val Loss: 1.8057 | Val Acc: 62.20%\n",
      "  ‚úÖ Best model saved! (Val Acc: 62.20%)\n",
      "Epoch [ 7/30] | Train Loss: 3.1365 | Train Acc: 62.50% | Val Loss: 1.6283 | Val Acc: 64.63%\n",
      "  ‚úÖ Best model saved! (Val Acc: 64.63%)\n",
      "Epoch [ 8/30] | Train Loss: 2.9138 | Train Acc: 72.26% | Val Loss: 1.4271 | Val Acc: 73.17%\n",
      "  ‚úÖ Best model saved! (Val Acc: 73.17%)\n",
      "Epoch [ 9/30] | Train Loss: 2.6813 | Train Acc: 76.22% | Val Loss: 1.2011 | Val Acc: 76.83%\n",
      "  ‚úÖ Best model saved! (Val Acc: 76.83%)\n",
      "Epoch [10/30] | Train Loss: 2.4648 | Train Acc: 79.27% | Val Loss: 1.0216 | Val Acc: 80.49%\n",
      "  ‚úÖ Best model saved! (Val Acc: 80.49%)\n",
      "Epoch [11/30] | Train Loss: 2.3152 | Train Acc: 83.23% | Val Loss: 0.9065 | Val Acc: 80.49%\n",
      "Epoch [12/30] | Train Loss: 2.1028 | Train Acc: 90.55% | Val Loss: 0.7838 | Val Acc: 85.37%\n",
      "  ‚úÖ Best model saved! (Val Acc: 85.37%)\n",
      "Epoch [13/30] | Train Loss: 1.9911 | Train Acc: 90.55% | Val Loss: 0.7190 | Val Acc: 85.37%\n",
      "Epoch [14/30] | Train Loss: 1.8356 | Train Acc: 95.12% | Val Loss: 0.6371 | Val Acc: 84.15%\n",
      "Epoch [15/30] | Train Loss: 1.8150 | Train Acc: 93.90% | Val Loss: 0.5903 | Val Acc: 87.80%\n",
      "  ‚úÖ Best model saved! (Val Acc: 87.80%)\n",
      "Epoch [16/30] | Train Loss: 1.7026 | Train Acc: 96.65% | Val Loss: 0.5612 | Val Acc: 86.59%\n",
      "Epoch [17/30] | Train Loss: 1.6371 | Train Acc: 96.04% | Val Loss: 0.5044 | Val Acc: 87.80%\n",
      "Epoch [18/30] | Train Loss: 1.6134 | Train Acc: 99.39% | Val Loss: 0.5044 | Val Acc: 86.59%\n",
      "Epoch [19/30] | Train Loss: 1.5999 | Train Acc: 97.87% | Val Loss: 0.4982 | Val Acc: 85.37%\n",
      "Epoch [20/30] | Train Loss: 1.5216 | Train Acc: 99.09% | Val Loss: 0.4593 | Val Acc: 87.80%\n",
      "Epoch [21/30] | Train Loss: 1.5119 | Train Acc: 98.78% | Val Loss: 0.4307 | Val Acc: 89.02%\n",
      "  ‚úÖ Best model saved! (Val Acc: 89.02%)\n",
      "Epoch [22/30] | Train Loss: 1.5204 | Train Acc: 98.48% | Val Loss: 0.4266 | Val Acc: 89.02%\n",
      "Epoch [23/30] | Train Loss: 1.4921 | Train Acc: 98.17% | Val Loss: 0.3874 | Val Acc: 89.02%\n",
      "Epoch [24/30] | Train Loss: 1.4641 | Train Acc: 99.39% | Val Loss: 0.3900 | Val Acc: 89.02%\n",
      "Epoch [25/30] | Train Loss: 1.4660 | Train Acc: 99.39% | Val Loss: 0.4017 | Val Acc: 90.24%\n",
      "  ‚úÖ Best model saved! (Val Acc: 90.24%)\n",
      "Epoch [26/30] | Train Loss: 1.4382 | Train Acc: 100.00% | Val Loss: 0.3929 | Val Acc: 89.02%\n",
      "Epoch [27/30] | Train Loss: 1.4199 | Train Acc: 100.00% | Val Loss: 0.3587 | Val Acc: 89.02%\n",
      "Epoch [28/30] | Train Loss: 1.4582 | Train Acc: 99.70% | Val Loss: 0.3566 | Val Acc: 89.02%\n",
      "Epoch [29/30] | Train Loss: 1.4171 | Train Acc: 99.39% | Val Loss: 0.3381 | Val Acc: 87.80%\n",
      "Epoch [30/30] | Train Loss: 1.3853 | Train Acc: 100.00% | Val Loss: 0.3543 | Val Acc: 90.24%\n",
      "\n",
      "============================================================\n",
      "‚úÖ Training completed successfully!\n",
      "üèÜ Best Validation Accuracy: 90.24%\n",
      "üíæ Model saved as 'inception_v1_stanford_cars_20classes.pth'\n",
      "============================================================\n",
      "\n",
      "üß™ Evaluating on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_14340\\335444959.py:217: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(MODEL_SAVE_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Test Loss: 0.3582 | Test Accuracy: 88.18%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from data_preprocessing import get_dataloaders\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_SAVE_PATH = \"inception_v1_stanford_cars_10classes.pth\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10  # Updated to 20 classes\n",
    "NUM_EPOCHS = 30\n",
    "IMG_SIZE = 299  # Inception requires 299x299 input\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL ARCHITECTURE\n",
    "# ==========================================\n",
    "def get_inception_model(num_classes=10):\n",
    "    # Load Pre-trained Inception V1 (GoogLeNet) with auxiliary classifiers enabled\n",
    "    model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1, aux_logits=True)\n",
    "\n",
    "    # --- STRATEGY: FINE-TUNING ---\n",
    "    # 1. Freeze the early layers (generic features like lines/edges)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # 2. Unfreeze the last inception modules for better feature learning\n",
    "    # GoogLeNet has inception4 and inception5 modules\n",
    "    for param in model.inception4e.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.inception5a.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.inception5b.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # 3. Replace the Final Classifier Head\n",
    "    in_features = model.fc.in_features  # GoogLeNet has 1024 features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1024, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Replace auxiliary classifiers to match our number of classes\n",
    "    model.aux1.fc2 = nn.Linear(model.aux1.fc2.in_features, num_classes)\n",
    "    model.aux2.fc2 = nn.Linear(model.aux2.fc2.in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# 3. VALIDATION FUNCTION\n",
    "# ==========================================\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "    \n",
    "    Returns:\n",
    "        val_loss: Average validation loss\n",
    "        val_acc: Validation accuracy (%)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Handle potential tuple output from GoogLeNet\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # Use only the main output\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_acc = 100 * correct_predictions / total_samples\n",
    "    \n",
    "    return val_loss, val_acc\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING WITH VALIDATION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ Inception V1 (GoogLeNet) Training on 20 Random Classes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # 1. Load Data using HF pipeline (handles downloading, splitting, and transforms)\n",
    "        print(\"\\nüì¶ Loading data from Hugging Face Hub...\")\n",
    "        train_dl, val_dl, test_dl, selected_classes, label_mapping = get_dataloaders(\n",
    "            batch_size=BATCH_SIZE, \n",
    "            img_size=IMG_SIZE,  # Inception requires 299x299\n",
    "            num_workers=0,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéØ Selected Classes: {selected_classes}\")\n",
    "        print(f\"üìä Number of Classes: {len(selected_classes)}\")\n",
    "        \n",
    "        print(\"\\nü§ñ Initializing Inception V1 (GoogLeNet)...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"üíª Using device: {device}\")\n",
    "        \n",
    "        model = get_inception_model(num_classes=NUM_CLASSES).to(device)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "        \n",
    "        # Learning Rate Scheduler (based on validation loss)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.1, patience=3, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Track best validation accuracy for model saving\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        print(f\"\\nüèãÔ∏è Starting Training for {NUM_EPOCHS} epochs...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # ==========================================\n",
    "            # TRAINING PHASE\n",
    "            # ==========================================\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0 \n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_dl:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # GoogLeNet returns (outputs, aux2, aux1) during training when aux_logits=True\n",
    "                outputs = model(images)\n",
    "                \n",
    "                if isinstance(outputs, tuple):\n",
    "                    # Training mode: unpack main output and auxiliary outputs\n",
    "                    main_output, aux2_output, aux1_output = outputs\n",
    "                    loss1 = criterion(main_output, labels)\n",
    "                    loss2 = criterion(aux2_output, labels)\n",
    "                    loss3 = criterion(aux1_output, labels)\n",
    "                    # Combined loss: main output weighted more heavily\n",
    "                    loss = loss1 + 0.3 * loss2 + 0.3 * loss3\n",
    "                    outputs = main_output  # Use main output for accuracy calculation\n",
    "                else:\n",
    "                    # Inference mode: single output\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Track Training Accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_epoch_loss = train_loss / len(train_dl)\n",
    "            train_epoch_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            # ==========================================\n",
    "            # VALIDATION PHASE\n",
    "            # ==========================================\n",
    "            val_loss, val_acc = validate_model(model, val_dl, criterion, device)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Epoch [{epoch+1:2d}/{NUM_EPOCHS}] | \"\n",
    "                  f\"Train Loss: {train_epoch_loss:.4f} | Train Acc: {train_epoch_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Update learning rate based on validation loss\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Save best model based on validation accuracy\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'selected_classes': selected_classes,\n",
    "                    'label_mapping': label_mapping\n",
    "                }, MODEL_SAVE_PATH)\n",
    "                print(f\"  ‚úÖ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        print(f\"üèÜ Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"üíæ Model saved as '{MODEL_SAVE_PATH}'\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # ==========================================\n",
    "        # FINAL TEST EVALUATION\n",
    "        # ==========================================\n",
    "        print(\"\\nüß™ Evaluating on Test Set...\")\n",
    "        checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        test_loss, test_acc = validate_model(model, test_dl, criterion, device)\n",
    "        print(f\"üìà Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n‚ùå An error occurred during execution:\")\n",
    "        print(e)\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36389a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
